{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # provide sql-like data manipulation tools. very handy.\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np # high dimensional vector computing library.\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec # the word2vec model gensim class\n",
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence # we'll talk about this down below\n",
    "\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # a tweet tokenizer from nltk.\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'Sentiment'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-713e5d570d2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mingest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-105-713e5d570d2f>\u001b[0m in \u001b[0;36mingest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'SentimentText'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSentiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m#data['Sentiment'] = data['Sentiment'].map( {4:1, 0:0} )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SentimentText'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jatingarg/anaconda/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, arg, na_action)\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2157\u001b[0m             \u001b[0;31m# arg is a function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2158\u001b[0;31m             \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m         return self._constructor(new_values,\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Sentiment'"
     ]
    }
   ],
   "source": [
    "def ingest():\n",
    "    data = pd.read_csv('training_1_sentiment_0.csv', encoding = \"ISO-8859-1\")\n",
    "\n",
    "    data1 = pd.read_csv('training_2_sentiment_0.csv', encoding = \"ISO-8859-1\")\n",
    "    data2 = pd.read_csv('violent_tweets_sentiment_1.csv', encoding = \"ISO-8859-1\")\n",
    "    data3 = pd.read_csv('rapeTweets.csv', encoding = \"ISO-8859-1\")\n",
    "    data3[['Sentiment']] = data3[['Sentiment']].apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "    data = data.append(data1)\n",
    "    data = data.append(data2)\n",
    "    data = data.append(data3)\n",
    "    #remove extra columns\n",
    "#     data.drop(['Date'], axis=1, inplace=True)\n",
    "    data = data.filter(['Sentiment','SentimentText'])\n",
    "    data = data[data.Sentiment.isnull() == False]\n",
    "    data['Sentiment'] = data['Sentiment'].map(int)\n",
    "    #data['Sentiment'] = data['Sentiment'].map( {4:1, 0:0} )\n",
    "    data = data[data['SentimentText'].isnull() == False]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    print ('dataset loaded with shape', data.shape)   \n",
    "    return data\n",
    "\n",
    "data = ingest()\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make a better classifier\n",
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet = tweet.lower()\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = list(filter(lambda t: not t.startswith('@'), tokens))\n",
    "        tokens = list(filter(lambda t: not t.startswith('#'), tokens))\n",
    "        tokens = list(filter(lambda t: not t.startswith('http'), tokens))\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def postprocess(data, n=1000000):\n",
    "    data = data.head(n)\n",
    "    data['tokens'] = data['SentimentText'].progress_map(tokenize)  ## progress_map is a variant of the map function plus a progress bar. Handy to monitor DataFrame creations.\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 57047/57047 [00:05<00:00, 11166.24it/s]\n"
     ]
    }
   ],
   "source": [
    "data = postprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>@RealPatriot1976 @4everNeverTrump @MooreSenate...</td>\n",
       "      <td>[you, forgot, \", thinks, pre-school, is, a, na...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @boell_etics: En 2015 se adoptaron las regl...</td>\n",
       "      <td>[rt, :, en, 2015, se, adoptaron, las, reglas, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @Forbes: How To Look Like A Leader When You...</td>\n",
       "      <td>[rt, :, how, to, look, like, a, leader, when, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @SteveScalise: Our troops deserve it! https...</td>\n",
       "      <td>[rt, :, our, troops, deserve, it, !]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>RT @LaScaldaferri: Smash cut to people standin...</td>\n",
       "      <td>[rt, :, smash, cut, to, people, standing, in, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText  \\\n",
       "0          0  @RealPatriot1976 @4everNeverTrump @MooreSenate...   \n",
       "1          0  RT @boell_etics: En 2015 se adoptaron las regl...   \n",
       "2          0  RT @Forbes: How To Look Like A Leader When You...   \n",
       "3          0  RT @SteveScalise: Our troops deserve it! https...   \n",
       "4          0  RT @LaScaldaferri: Smash cut to people standin...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [you, forgot, \", thinks, pre-school, is, a, na...  \n",
       "1  [rt, :, en, 2015, se, adoptaron, las, reglas, ...  \n",
       "2  [rt, :, how, to, look, like, a, leader, when, ...  \n",
       "3               [rt, :, our, troops, deserve, it, !]  \n",
       "4  [rt, :, smash, cut, to, people, standing, in, ...  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=1000000\n",
    "x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(n).tokens),\n",
    "                                                    np.array(data.head(n).Sentiment), test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['roads',\n",
       " ',',\n",
       " 'schools',\n",
       " ',',\n",
       " 'defense',\n",
       " ',',\n",
       " 'weather',\n",
       " ',',\n",
       " 'research',\n",
       " ',',\n",
       " 'healthcare',\n",
       " 'are',\n",
       " 'all',\n",
       " 'waste',\n",
       " ';',\n",
       " 'because',\n",
       " 'you',\n",
       " '\\\\',\n",
       " 'u2026']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3092475"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_dim=200\n",
    "token_count = sum([len(sentence) for sentence in x_train])\n",
    "tweet_w2v = Word2Vec(size=n_dim, min_count=10)\n",
    "tweet_w2v.build_vocab(x_train)\n",
    "#tweet_w2v.train(x_train, total_examples=token_count, epochs=15)\n",
    "tweet_w2v.train(x_train, total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter)\n",
    "#total_examples=tweet_w2v.corpus_count, epochs=tweet_w2v.iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jatingarg/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ -2.62365103e-01,  -1.22907841e+00,   3.05420667e-01,\n",
       "         5.35618424e-01,   4.90612209e-01,   1.07360184e+00,\n",
       "        -3.15761864e-01,   1.32183847e-03,  -1.64737141e+00,\n",
       "        -3.48463178e-01,   1.16813409e+00,   2.28720784e-01,\n",
       "         2.61147976e-01,  -1.55384743e+00,  -7.40929306e-01,\n",
       "         5.20689189e-01,  -9.23383117e-01,   1.75207049e-01,\n",
       "         6.49505496e-01,   5.08321166e-01,   1.45856693e-01,\n",
       "         3.99922729e-01,  -1.42671013e+00,  -3.18788677e-01,\n",
       "        -5.79857945e-01,  -2.03953072e-01,   3.89585346e-01,\n",
       "         9.66223478e-01,  -3.35735381e-01,  -3.54420304e-01,\n",
       "         5.31349838e-01,   9.23538864e-01,  -2.80488819e-01,\n",
       "        -6.54184341e-01,   6.21118248e-01,  -4.48435396e-01,\n",
       "        -5.98192692e-01,  -6.90549791e-01,  -7.86706924e-01,\n",
       "        -1.02881871e-01,  -4.43934090e-02,   4.03930932e-01,\n",
       "        -4.79687214e-01,  -3.02368373e-01,  -7.74409294e-01,\n",
       "        -8.43152642e-01,  -9.27366734e-01,  -2.44783945e-02,\n",
       "         6.24480247e-01,  -3.75782609e-01,   2.27934003e-01,\n",
       "        -2.93347508e-01,  -6.25837386e-01,   1.98342755e-01,\n",
       "        -1.99881256e-01,   1.01648521e+00,  -5.62649965e-01,\n",
       "        -2.37614647e-01,  -9.42024410e-01,  -1.80487372e-02,\n",
       "        -8.97309557e-02,   4.43153769e-01,  -6.80577934e-01,\n",
       "         8.08596492e-01,   1.84169799e-01,   1.30815494e+00,\n",
       "         2.76153356e-01,   6.76558763e-02,   5.10244370e-01,\n",
       "         5.23283780e-01,  -5.39841317e-02,   3.70959222e-01,\n",
       "         4.90738332e-01,  -4.19620156e-01,   3.98152083e-01,\n",
       "         1.33213952e-01,   2.81487167e-01,  -8.48818839e-01,\n",
       "        -9.70935643e-01,   7.23010957e-01,  -6.50019869e-02,\n",
       "        -5.07995784e-01,  -2.94269532e-01,   8.08921337e-01,\n",
       "         3.64126921e-01,   1.69941759e+00,  -2.53121138e-01,\n",
       "        -5.93305111e-01,  -5.18370390e-01,   5.77611566e-01,\n",
       "         1.85929909e-01,   2.86806464e-01,   8.13683808e-01,\n",
       "        -7.44902730e-01,   4.93980467e-01,  -2.47163877e-01,\n",
       "        -3.31372887e-01,   1.43040037e+00,   9.51233413e-03,\n",
       "        -9.85583007e-01,   1.17652953e+00,   9.69822034e-02,\n",
       "         8.05422515e-02,   5.07147789e-01,   1.20626581e+00,\n",
       "         4.73033279e-01,  -2.52395958e-01,  -5.63818514e-02,\n",
       "         4.86580938e-01,  -2.36045435e-01,  -5.26358150e-02,\n",
       "        -8.68109822e-01,  -1.22383857e+00,   4.28083390e-01,\n",
       "         7.19505906e-01,  -1.71402425e-01,   1.13533624e-02,\n",
       "        -7.40270972e-01,  -4.99171615e-01,  -7.55345106e-01,\n",
       "         1.08279979e+00,   4.40363526e-01,   1.14781356e+00,\n",
       "        -4.98713225e-01,   4.46010262e-01,   2.27279946e-01,\n",
       "         6.24917932e-02,  -3.75654370e-01,  -8.05391371e-01,\n",
       "        -1.30015087e+00,  -1.02457869e+00,   1.32109237e+00,\n",
       "        -4.70350444e-01,   1.39851022e+00,   7.68361688e-01,\n",
       "         9.04514253e-01,   4.05680053e-02,  -4.99933630e-01,\n",
       "         3.90433669e-01,  -3.24011415e-01,   1.63700807e+00,\n",
       "         5.60833439e-02,   3.24809514e-02,  -1.62981331e+00,\n",
       "         6.89617172e-03,  -7.58802176e-01,  -1.73882201e-01,\n",
       "        -3.37907732e-01,   2.48163119e-01,   4.36550856e-01,\n",
       "        -5.39493144e-01,   2.44566247e-01,   1.53364038e+00,\n",
       "        -1.00353909e+00,  -7.86638558e-01,   2.32839659e-01,\n",
       "         2.41169810e-01,  -3.39230299e-01,   4.34198618e-01,\n",
       "        -1.54047012e-01,  -6.04288638e-01,  -1.06786048e+00,\n",
       "        -1.30687952e-01,   2.59015352e-01,  -1.50774732e-01,\n",
       "        -1.39859423e-01,  -1.56392574e-01,  -1.03285158e+00,\n",
       "        -5.18129647e-01,  -3.51824701e-01,  -8.90033126e-01,\n",
       "        -4.53474998e-01,   6.68120861e-01,  -2.81340778e-01,\n",
       "         8.09750021e-01,   1.62187028e+00,   6.11869618e-02,\n",
       "         1.89340606e-01,   2.59683222e-01,   1.19418446e-02,\n",
       "        -8.32857490e-02,   8.68619680e-01,  -5.07104516e-01,\n",
       "         3.47020090e-01,   4.27842051e-01,   2.13767365e-01,\n",
       "        -1.69926047e-01,   1.66055068e-01,  -9.04217541e-01,\n",
       "         1.35959536e-01,   4.63229269e-01,   8.89881477e-02,\n",
       "         4.99234706e-01,  -8.36929381e-01,   8.38611066e-01,\n",
       "         1.16465282e+00,   2.00897366e-01,   1.50892287e-01,\n",
       "        -8.05767119e-01,  -1.20689742e-01], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v['good']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jatingarg/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('great', 0.8634505271911621),\n",
       " ('wonderful', 0.8304198384284973),\n",
       " ('happy', 0.805043637752533),\n",
       " ('fun', 0.770256519317627),\n",
       " ('let', 0.7636302709579468),\n",
       " ('long', 0.7626844644546509),\n",
       " ('. .', 0.7594529390335083),\n",
       " ('better', 0.7498566508293152),\n",
       " ('pretty', 0.7480359077453613),\n",
       " ('too', 0.747560441493988)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar('good')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jatingarg/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('cancer', 0.8544315099716187),\n",
       " ('undergoing', 0.8355053663253784),\n",
       " ('gangs', 0.8303781747817993),\n",
       " ('treatment', 0.8297376036643982),\n",
       " ('children', 0.8246831893920898),\n",
       " ('rohingya', 0.805081307888031),\n",
       " ('victims', 0.7936364412307739),\n",
       " ('develop', 0.7934758067131042),\n",
       " ('diabetes', 0.792036235332489),\n",
       " ('shots', 0.7877124547958374)]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar('rape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jatingarg/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('store', 0.87354576587677),\n",
       " ('dragon', 0.8732419013977051),\n",
       " ('mocking', 0.8677797317504883),\n",
       " ('32', 0.8674526214599609),\n",
       " ('trailer', 0.8566007614135742),\n",
       " ('china', 0.8544414043426514),\n",
       " ('cod', 0.8483848571777344),\n",
       " ('vlog', 0.8468364477157593),\n",
       " ('featuring', 0.8451340198516846),\n",
       " ('stars', 0.8435724973678589)]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_w2v.most_similar('facebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tweet_w2v.most_similar('ocean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport bokeh.plotting as bp\\nfrom bokeh.models import HoverTool, BoxSelectTool\\nfrom bokeh.plotting import figure, show, output_notebook\\n\\n# defining the chart\\noutput_notebook()\\nplot_tfidf = bp.figure(plot_width=700, plot_height=600, title=\"A map of 10000 word vectors\",\\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\\n    x_axis_type=None, y_axis_type=None, min_border=1)\\n\\n# getting a list of word vectors. limit to 10000. each is of 200 dimensions\\nword_vectors = [tweet_w2v[w] for w in list(tweet_w2v.wv.vocab.keys())[:5000]]\\n\\n# dimensionality reduction. converting the vectors to 2d vectors\\nfrom sklearn.manifold import TSNE\\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0)\\ntsne_w2v = tsne_model.fit_transform(word_vectors)\\n\\n# putting everything in a dataframe\\ntsne_df = pd.DataFrame(tsne_w2v, columns=[\\'x\\', \\'y\\'])\\ntsne_df[\\'words\\'] = list(tweet_w2v.wv.vocab.keys())[:5000]\\n\\n# plotting. the corresponding word appears when you hover on the data point.\\nplot_tfidf.scatter(x=\\'x\\', y=\\'y\\', source=tsne_df)\\nhover = plot_tfidf.select(dict(type=HoverTool))\\nhover.tooltips={\"word\": \"@words\"}\\nshow(plot_tfidf)\\n'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing bokeh library for interactive dataviz\n",
    "\"\"\"\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool, BoxSelectTool\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "\n",
    "# defining the chart\n",
    "output_notebook()\n",
    "plot_tfidf = bp.figure(plot_width=700, plot_height=600, title=\"A map of 10000 word vectors\",\n",
    "    tools=\"pan,wheel_zoom,box_zoom,reset,hover,previewsave\",\n",
    "    x_axis_type=None, y_axis_type=None, min_border=1)\n",
    "\n",
    "# getting a list of word vectors. limit to 10000. each is of 200 dimensions\n",
    "word_vectors = [tweet_w2v[w] for w in list(tweet_w2v.wv.vocab.keys())[:5000]]\n",
    "\n",
    "# dimensionality reduction. converting the vectors to 2d vectors\n",
    "from sklearn.manifold import TSNE\n",
    "tsne_model = TSNE(n_components=2, verbose=1, random_state=0)\n",
    "tsne_w2v = tsne_model.fit_transform(word_vectors)\n",
    "\n",
    "# putting everything in a dataframe\n",
    "tsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\n",
    "tsne_df['words'] = list(tweet_w2v.wv.vocab.keys())[:5000]\n",
    "\n",
    "# plotting. the corresponding word appears when you hover on the data point.\n",
    "plot_tfidf.scatter(x='x', y='y', source=tsne_df)\n",
    "hover = plot_tfidf.select(dict(type=HoverTool))\n",
    "hover.tooltips={\"word\": \"@words\"}\n",
    "show(plot_tfidf)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building tf-idf matrix ...\n",
      "vocab size : 7059\n"
     ]
    }
   ],
   "source": [
    "print ('building tf-idf matrix ...')\n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform(x_train)\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "print ('vocab size :', len(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Now let's define a function that, given a list of tweet tokens, creates an averaged tweet vector.\n",
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError: # handling the case where the token is not\n",
    "                         # in the corpus. useful for testing.\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jatingarg/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in x_train])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in x_test])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/9\n",
      " - 1s - loss: 0.2188 - acc: 0.9171\n",
      "Epoch 2/9\n",
      " - 1s - loss: 0.1889 - acc: 0.9327\n",
      "Epoch 3/9\n",
      " - 1s - loss: 0.1807 - acc: 0.9366\n",
      "Epoch 4/9\n",
      " - 1s - loss: 0.1747 - acc: 0.9381\n",
      "Epoch 5/9\n",
      " - 1s - loss: 0.1707 - acc: 0.9411\n",
      "Epoch 6/9\n",
      " - 1s - loss: 0.1683 - acc: 0.9409\n",
      "Epoch 7/9\n",
      " - 1s - loss: 0.1650 - acc: 0.9418\n",
      "Epoch 8/9\n",
      " - 1s - loss: 0.1623 - acc: 0.9435\n",
      "Epoch 9/9\n",
      " - 1s - loss: 0.1607 - acc: 0.9443\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a14534d68>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=200))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_vecs_w2v, y_train, epochs=9, batch_size=32, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941454864165\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(test_vecs_w2v, y_test, batch_size=128, verbose=2)\n",
    "print (score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded with shape (3947, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>\"You fuck your dad.\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>\"i really don't understand your point.\\xa0 It ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>\"A\\\\xc2\\\\xa0majority of Canadians can and has ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>\"listen if you dont wanna get married to a man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>\"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                      SentimentText\n",
       "0          1                               \"You fuck your dad.\"\n",
       "1          0  \"i really don't understand your point.\\xa0 It ...\n",
       "2          0  \"A\\\\xc2\\\\xa0majority of Canadians can and has ...\n",
       "3          0  \"listen if you dont wanna get married to a man...\n",
       "4          0  \"C\\xe1c b\\u1ea1n xu\\u1ed1ng \\u0111\\u01b0\\u1edd..."
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for checking on test data\n",
    "def ingest2():\n",
    "    data = pd.read_csv('train.csv', encoding = \"ISO-8859-1\")\n",
    "#     data.drop(['Date', 'Usage'], axis=1, inplace=True)\n",
    "    data = data.filter(['Sentiment','SentimentText'])\n",
    "    data = data[data.Sentiment.isnull() == False]\n",
    "    data['Sentiment'] = data['Sentiment'].map(int)\n",
    "    #data['Sentiment'] = data['Sentiment'].map( {4:1, 0:0} )\n",
    "    data = data[data['SentimentText'].isnull() == False]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    print ('dataset loaded with shape', data.shape)   \n",
    "    return data\n",
    "\n",
    "data2 = ingest2()\n",
    "data2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 3947/3947 [00:00<00:00, 6293.45it/s]\n"
     ]
    }
   ],
   "source": [
    "testData = postprocess(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n=1000000\n",
    "x_test_train, x_test, y_test_train, y_test = train_test_split(np.array(testData.head(n).tokens),\n",
    "                                                    np.array(testData.head(n).Sentiment), test_size=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jatingarg/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "test_train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in x_test_train])\n",
    "test_train_vecs_w2v = scale(test_train_vecs_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.61616417481\n"
     ]
    }
   ],
   "source": [
    "#Accuracy on test data\n",
    "score = model.evaluate(test_train_vecs_w2v, y_test_train, batch_size=128, verbose=2)\n",
    "print (score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.predict()   <- returns a list of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields = ['created_at', 'id_str','SentimentText', 'user-id_str', 'user-name', 'user-screen_name',\n",
    "          'user-location', 'user-url', 'user-description', 'user-protected', 'user-verified',\n",
    "          'user-followers_count', 'user-friends_count', 'user-listed_count', 'user-favourites_count',\n",
    "          'user-statuses_count', 'user-created_at', 'user-utc_offset', 'user-time_zone', 'user-geo_enabled',\n",
    "          'user-lang', 'user-following', 'geo', 'coordinates', 'place', 'contributors', 'is_quote_status',\n",
    "          'quote_count', 'reply_count', 'retweet_count', 'favorite_count', 'favorited', 'retweeted',\n",
    "          'filter_level', 'lang',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF = pd.read_csv(\"rawData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDF.columns = fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id_str</th>\n",
       "      <th>SentimentText</th>\n",
       "      <th>user-id_str</th>\n",
       "      <th>user-name</th>\n",
       "      <th>user-screen_name</th>\n",
       "      <th>user-location</th>\n",
       "      <th>user-url</th>\n",
       "      <th>user-description</th>\n",
       "      <th>user-protected</th>\n",
       "      <th>...</th>\n",
       "      <th>contributors</th>\n",
       "      <th>is_quote_status</th>\n",
       "      <th>quote_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>filter_level</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-12-14 21:01:44</td>\n",
       "      <td>941412915990749184</td>\n",
       "      <td>RT @mugen: girls after break up:\\n\\ndick appoi...</td>\n",
       "      <td>2604225667</td>\n",
       "      <td>Jolly Ty</td>\n",
       "      <td>tje___</td>\n",
       "      <td>Nine19 Raised</td>\n",
       "      <td>http://vsco.com/indigoty</td>\n",
       "      <td>I’m a disrespectful, but i’m woke. #blm #ncat</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>low</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            created_at              id_str  \\\n",
       "0  2017-12-14 21:01:44  941412915990749184   \n",
       "\n",
       "                                       SentimentText  user-id_str user-name  \\\n",
       "0  RT @mugen: girls after break up:\\n\\ndick appoi...   2604225667  Jolly Ty   \n",
       "\n",
       "  user-screen_name  user-location                  user-url  \\\n",
       "0           tje___  Nine19 Raised  http://vsco.com/indigoty   \n",
       "\n",
       "                                user-description  user-protected  ...   \\\n",
       "0  I’m a disrespectful, but i’m woke. #blm #ncat           False  ...    \n",
       "\n",
       "   contributors  is_quote_status  quote_count  reply_count  retweet_count  \\\n",
       "0           NaN            False            0            0              0   \n",
       "\n",
       "   favorite_count favorited  retweeted filter_level  lang  \n",
       "0               0     False      False          low    en  \n",
       "\n",
       "[1 rows x 35 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testDF = testDF.filter(['SentimentText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @mugen: girls after break up:\\n\\ndick appoi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       SentimentText\n",
       "0  RT @mugen: girls after break up:\\n\\ndick appoi..."
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testDF.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
